[args]
bert_model=./roberta-base
root_data_dir=datasets_with_WPDom/verbparse_deparser_domain_scores_k_fold/
model=WPDM
mlp_layers=1
task_name=MOH-X
word_type=verbal
classifier_hidden=768
label_size=2
lr_schedule=warmup
warmup_epoch=4
drop_ratio=0.5
kfold=10
max_seq_length=160
do_train=True
do_test=True
do_eval=True
do_lower_case=True
hidden_size=768
train_batch_size=32
eval_batch_size=32
num_train_epoch=15
grad_clip_norm=1.0
save_checkpoint=False
save_best_model=False
model_root_path=saves/
seed=664
temp=0.05
learning_rate=0.003
weight_decay=0.01
optimizer=Adam
module_name=encoder, classifier
module_lr=(3e-05, 0.003)
module_weight_decay=(0.01, 0.01)
word_emb_type=mean
word_pair_max=5
segment=none
spec_tok=False
cls_num=3
tau_max=10
attention_layer=2
ffn_size=768
multihead=4
average=False
main_conf_path=
model_path=saves/MOH-X/verbal
data_dir=datasets_with_WPDom/verbparse_deparser_domain_scores_k_fold/MOH-X


